{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\testenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import neighbors\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargarDatos():\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_dev = []\n",
    "    y_dev = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    with open(\"data/training.txt\") as f:\n",
    "        datos = f.read()\n",
    "        for tweet in datos.split(\"\\n\"):\n",
    "            features = tweet.split(\" \")\n",
    "            for feature in features:\n",
    "                if feature==features[1]:\n",
    "                    y_train.append(feature)\n",
    "                if feature==features[2]:\n",
    "                    break\n",
    "            contenido = ''\n",
    "            for feature in features[2:]:\n",
    "                contenido += feature + ' '\n",
    "            contenido = contenido[:-1]\n",
    "            x_train.append(contenido)\n",
    "\n",
    "    with open(\"data/development.txt\") as f:\n",
    "        datos = f.read()\n",
    "        for tweet in datos.split(\"\\n\"):\n",
    "            features = tweet.split(\" \")\n",
    "            for feature in features:\n",
    "                if feature==features[1]:\n",
    "                    y_dev.append(feature)\n",
    "                if feature==features[2]:\n",
    "                    break\n",
    "            contenido = ''\n",
    "            for feature in features[2:]:\n",
    "                contenido += feature + ' '\n",
    "            contenido = contenido[:-1]\n",
    "            x_dev.append(contenido)\n",
    "    \n",
    "    with open(\"data/test_alumnes.txt\") as f:\n",
    "        datos = f.read()\n",
    "        for tweet in datos.split(\"\\n\"):\n",
    "            features = tweet.split(\" \")\n",
    "            for feature in features:\n",
    "                if feature==features[0]:\n",
    "                    y_test.append(feature)\n",
    "                if feature==features[1]:\n",
    "                    break\n",
    "            contenido = ''\n",
    "            for feature in features[2:]:\n",
    "                contenido += feature + ' '\n",
    "            contenido = contenido[:-1]\n",
    "            x_test.append(contenido)\n",
    "                    \n",
    "    return x_train, y_train, x_dev, y_dev, x_test, y_test\n",
    "\n",
    "def preproceso(x_train, x_dev, x_test):\n",
    "    train = []\n",
    "    dev = []\n",
    "    test = []\n",
    "\n",
    "    reMencionHashtag = re.compile(r'@+\\w+' + '|' + '#+\\w+')\n",
    "    reWeb = re.compile(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+')\n",
    "\n",
    "    print('Limpiamos los datos...')\n",
    "    for element in x_train:\n",
    "        for item in re.finditer(reMencionHashtag, element):\n",
    "            element = reMencionHashtag.sub('>tweet', element)\n",
    "        for item in re.finditer(reWeb, element):\n",
    "            element = reWeb.sub('>url', element)\n",
    "        train.append(element)\n",
    "\n",
    "    for element in x_dev:\n",
    "        for item in re.finditer(reMencionHashtag, element):\n",
    "            element = reMencionHashtag.sub('>tweet', element)\n",
    "        for item in re.finditer(reWeb, element):\n",
    "            element = reWeb.sub('>url', element)\n",
    "        dev.append(element)\n",
    "\n",
    "    for element in x_test:\n",
    "        for item in re.finditer(reMencionHashtag, element):\n",
    "            element = reMencionHashtag.sub('>tweet', element)\n",
    "        for item in re.finditer(reWeb, element):\n",
    "            element = reWeb.sub('>url', element)\n",
    "        test.append(element)\n",
    "\n",
    "    print('Vectorizamos...')\n",
    "    vectorizador = TfidfVectorizer(tokenizer=casual_tokenize, max_df=0.8)\n",
    "    vectorizador.fit(train)\n",
    "    vectorizador.fit_transform(dev)\n",
    "    vectorizador.fit_transform(test)\n",
    "    matriz_train = vectorizador.transform(train)\n",
    "    matriz_dev = vectorizador.transform(dev)\n",
    "    matriz_test = vectorizador.transform(test)\n",
    "\n",
    "    return matriz_train.toarray(), matriz_dev.toarray(), matriz_test.toarray()\n",
    "\n",
    "def train(matriz_train, y_train):\n",
    "    #modelo = svm.SVC(C=1)\n",
    "    #modelo = svm.LinearSVC(C=1)\n",
    "    modelo = svm.LinearSVC(C=100, tol=0.1, loss='hinge', max_iter=1000000000)\n",
    "    #modelo = GaussianNB()\n",
    "    #modelo = GradientBoostingClassifier()\n",
    "    #modelo = SGDClassifier()\n",
    "    #modelo = neighbors.KNeighborsClassifier()\n",
    "    \n",
    "    modelo.fit(matriz_train, y_train)\n",
    "\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargamos los datos...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCargamos los datos...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m x_train, y_train, x_dev, y_dev, x_test, y_test \u001b[39m=\u001b[39m cargarDatos()\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPreprocesamos los datos...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m matriz_train, matriz_dev, matriz_test \u001b[39m=\u001b[39m preproceso(x_train, x_dev, x_test)\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mcargarDatos\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m features \u001b[39m=\u001b[39m tweet\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features:\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mif\u001b[39;00m feature\u001b[39m==\u001b[39mfeatures[\u001b[39m1\u001b[39;49m]:\n\u001b[0;32m     15\u001b[0m         y_train\u001b[39m.\u001b[39mappend(feature)\n\u001b[0;32m     16\u001b[0m     \u001b[39mif\u001b[39;00m feature\u001b[39m==\u001b[39mfeatures[\u001b[39m2\u001b[39m]:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('Cargamos los datos...')\n",
    "x_train, y_train, x_dev, y_dev, x_test, y_test = cargarDatos()\n",
    "\n",
    "print('Preprocesamos los datos...')\n",
    "matriz_train, matriz_dev, matriz_test = preproceso(x_train, x_dev, x_test)\n",
    "\n",
    "print('Entrenamos el modelo...')\n",
    "modelo = train(matriz_train, y_train)\n",
    "\n",
    "print('Realizamos las predicciones...')\n",
    "predicciones = modelo.predict(matriz_dev)\n",
    "\n",
    "print(\"precision = \", accuracy_score(y_dev, predicciones))\n",
    "print(\"macro promedio = \", precision_recall_fscore_support(y_dev, predicciones, average='macro'))\n",
    "print(\"micro promedio = \", precision_recall_fscore_support(y_dev, predicciones, average='micro'))\n",
    "\n",
    "resultados_test = modelo.predict(matriz_test)\n",
    "\n",
    "with open('svm.txt', 'a') as file:\n",
    "    for i in range(len(y_test)):\n",
    "        ids = y_test[i].split('\\n')\n",
    "        etiqueta = resultados_test[i].split('\\n')\n",
    "        file.write(ids[0]+'\\t'+etiqueta[0]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

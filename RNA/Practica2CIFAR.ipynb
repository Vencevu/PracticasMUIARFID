{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":422,"status":"ok","timestamp":1673515867121,"user":{"displayName":"Vencevú '_'","userId":"04987947938800528309"},"user_tz":-60},"id":"M-icrPLi3Gsq"},"outputs":[],"source":["from __future__ import print_function\n","\n","import keras\n","from keras.datasets import cifar10\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization as BN, GaussianNoise as GN,  Dense, Activation, Flatten, Dropout as DP\n","from tensorflow.keras.optimizers import SGD, Adamax, Adam\n","from keras.utils import np_utils\n","\n","from keras.callbacks import LearningRateScheduler as LRS, ReduceLROnPlateau as RLROP\n","from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1673515867122,"user":{"displayName":"Vencevú '_'","userId":"04987947938800528309"},"user_tz":-60},"id":"BP7O-h1x3s1D"},"outputs":[],"source":["batch_size = 64\n","num_classes = 10\n","epochs = 300\n","dropout = 0.2"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1323,"status":"ok","timestamp":1673515868442,"user":{"displayName":"Vencevú '_'","userId":"04987947938800528309"},"user_tz":-60},"id":"VmaFWFAn3yVy","outputId":"e063b764-701e-48ed-cbb4-d3cefa969abc"},"outputs":[{"name":"stdout","output_type":"stream","text":["(50000, 32, 32, 3)\n","(10000, 32, 32, 3)\n"]}],"source":["#### LOAD AND TRANSFORM\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","x_train /= 255\n","x_test /= 255\n","\n","print(x_train.shape)\n","print(x_test.shape)\n","\n","y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1673515868444,"user":{"displayName":"Vencevú '_'","userId":"04987947938800528309"},"user_tz":-60},"id":"jtWspKGq3zdB"},"outputs":[],"source":["## DEFINE A DATA AUGMENTATION GENERATOR\n","\n","datagen = ImageDataGenerator(\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    rotation_range=20,\n","    zoom_range=[1.0,1.2],\n","    shear_range = 15,\n","    horizontal_flip=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lIZha9hs35ty","outputId":"8fc328a5-477f-452c-b342-618225a80d1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 32, 32, 32)        896       \n","                                                                 \n"," batch_normalization (BatchN  (None, 32, 32, 32)       128       \n"," ormalization)                                                   \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 32, 32, 32)        9248      \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 32, 32, 32)       128       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 16, 16, 32)        0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 16, 16, 64)        18496     \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 16, 16, 64)        36928     \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 16, 16, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 16, 16, 64)        36928     \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 16, 16, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 8, 8, 128)         73856     \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 8, 8, 128)        512       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 8, 8, 128)         147584    \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 8, 8, 128)        512       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 8, 8, 128)         147584    \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 8, 8, 128)        512       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 4, 4, 256)         295168    \n","                                                                 \n"," batch_normalization_9 (Batc  (None, 4, 4, 256)        1024      \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_10 (Conv2D)          (None, 4, 4, 256)         590080    \n","                                                                 \n"," batch_normalization_10 (Bat  (None, 4, 4, 256)        1024      \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 4, 4, 256)         590080    \n","                                                                 \n"," batch_normalization_11 (Bat  (None, 4, 4, 256)        1024      \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 2, 2, 256)        0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_3 (Dropout)         (None, 2, 2, 256)         0         \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 2, 2, 512)         1180160   \n","                                                                 \n"," batch_normalization_12 (Bat  (None, 2, 2, 512)        2048      \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_13 (Conv2D)          (None, 2, 2, 512)         2359808   \n","                                                                 \n"," batch_normalization_13 (Bat  (None, 2, 2, 512)        2048      \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_14 (Conv2D)          (None, 2, 2, 512)         2359808   \n","                                                                 \n"," batch_normalization_14 (Bat  (None, 2, 2, 512)        2048      \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 1, 1, 512)        0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_4 (Dropout)         (None, 1, 1, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 512)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               262656    \n","                                                                 \n"," batch_normalization_15 (Bat  (None, 512)              2048      \n"," chNormalization)                                                \n","                                                                 \n"," dropout_5 (Dropout)         (None, 512)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                5130      \n","                                                                 \n","=================================================================\n","Total params: 8,137,610\n","Trainable params: 8,130,634\n","Non-trainable params: 6,976\n","_________________________________________________________________\n","Epoch 1/300\n","781/781 [==============================] - 29s 28ms/step - loss: 2.1161 - accuracy: 0.2723 - val_loss: 2.1206 - val_accuracy: 0.3024 - lr: 0.0010\n","Epoch 2/300\n","781/781 [==============================] - 20s 26ms/step - loss: 1.6311 - accuracy: 0.4118 - val_loss: 1.9756 - val_accuracy: 0.4009 - lr: 0.0010\n","Epoch 3/300\n","781/781 [==============================] - 20s 26ms/step - loss: 1.4611 - accuracy: 0.4814 - val_loss: 4.1300 - val_accuracy: 0.2462 - lr: 0.0010\n","Epoch 4/300\n","781/781 [==============================] - 20s 26ms/step - loss: 1.3196 - accuracy: 0.5352 - val_loss: 2.2744 - val_accuracy: 0.3954 - lr: 0.0010\n","Epoch 5/300\n","781/781 [==============================] - 20s 26ms/step - loss: 1.2197 - accuracy: 0.5763 - val_loss: 1.0483 - val_accuracy: 0.6392 - lr: 0.0010\n","Epoch 6/300\n","781/781 [==============================] - 20s 26ms/step - loss: 1.1445 - accuracy: 0.6055 - val_loss: 1.0906 - val_accuracy: 0.6282 - lr: 0.0010\n","Epoch 7/300\n","781/781 [==============================] - 20s 26ms/step - loss: 1.0720 - accuracy: 0.6324 - val_loss: 1.0680 - val_accuracy: 0.6387 - lr: 0.0010\n","Epoch 8/300\n","781/781 [==============================] - 20s 26ms/step - loss: 1.0197 - accuracy: 0.6535 - val_loss: 0.9798 - val_accuracy: 0.6727 - lr: 0.0010\n","Epoch 9/300\n","781/781 [==============================] - 20s 26ms/step - loss: 0.9685 - accuracy: 0.6752 - val_loss: 1.0759 - val_accuracy: 0.6546 - lr: 0.0010\n","Epoch 10/300\n","781/781 [==============================] - 20s 26ms/step - loss: 0.9228 - accuracy: 0.6937 - val_loss: 0.8699 - val_accuracy: 0.7105 - lr: 0.0010\n","Epoch 11/300\n","781/781 [==============================] - 20s 26ms/step - loss: 0.8897 - accuracy: 0.7037 - val_loss: 0.7981 - val_accuracy: 0.7294 - lr: 0.0010\n","Epoch 12/300\n","781/781 [==============================] - 20s 26ms/step - loss: 0.8467 - accuracy: 0.7182 - val_loss: 0.8519 - val_accuracy: 0.7371 - lr: 0.0010\n","Epoch 13/300\n","781/781 [==============================] - 21s 26ms/step - loss: 0.8083 - accuracy: 0.7319 - val_loss: 0.7196 - val_accuracy: 0.7641 - lr: 0.0010\n","Epoch 14/300\n","781/781 [==============================] - 24s 30ms/step - loss: 0.7809 - accuracy: 0.7407 - val_loss: 0.7889 - val_accuracy: 0.7388 - lr: 0.0010\n","Epoch 15/300\n","781/781 [==============================] - 22s 28ms/step - loss: 0.7671 - accuracy: 0.7459 - val_loss: 0.7470 - val_accuracy: 0.7626 - lr: 0.0010\n","Epoch 16/300\n","781/781 [==============================] - 23s 29ms/step - loss: 0.7372 - accuracy: 0.7557 - val_loss: 0.6198 - val_accuracy: 0.7974 - lr: 0.0010\n","Epoch 17/300\n","781/781 [==============================] - 44s 57ms/step - loss: 0.7135 - accuracy: 0.7648 - val_loss: 0.6022 - val_accuracy: 0.8010 - lr: 0.0010\n","Epoch 18/300\n","781/781 [==============================] - 23s 29ms/step - loss: 0.6987 - accuracy: 0.7681 - val_loss: 0.6837 - val_accuracy: 0.7803 - lr: 0.0010\n","Epoch 19/300\n","781/781 [==============================] - 21s 26ms/step - loss: 0.6826 - accuracy: 0.7753 - val_loss: 0.6127 - val_accuracy: 0.7961 - lr: 0.0010\n","Epoch 20/300\n","781/781 [==============================] - 25s 32ms/step - loss: 0.6721 - accuracy: 0.7782 - val_loss: 0.6131 - val_accuracy: 0.7982 - lr: 0.0010\n","Epoch 21/300\n","781/781 [==============================] - 20s 26ms/step - loss: 0.6466 - accuracy: 0.7865 - val_loss: 0.5533 - val_accuracy: 0.8177 - lr: 0.0010\n","Epoch 22/300\n","781/781 [==============================] - 20s 26ms/step - loss: 0.6294 - accuracy: 0.7890 - val_loss: 0.6166 - val_accuracy: 0.7978 - lr: 0.0010\n","Epoch 23/300\n","781/781 [==============================] - 20s 26ms/step - loss: 0.6227 - accuracy: 0.7943 - val_loss: 0.6233 - val_accuracy: 0.7995 - lr: 0.0010\n","Epoch 24/300\n","781/781 [==============================] - 20s 26ms/step - loss: 0.6187 - accuracy: 0.7965 - val_loss: 0.5060 - val_accuracy: 0.8353 - lr: 0.0010\n","Epoch 25/300\n","781/781 [==============================] - 21s 26ms/step - loss: 0.6045 - accuracy: 0.8020 - val_loss: 0.5552 - val_accuracy: 0.8183 - lr: 0.0010\n","Epoch 26/300\n","781/781 [==============================] - 21s 26ms/step - loss: 0.6006 - accuracy: 0.8036 - val_loss: 0.5354 - val_accuracy: 0.8217 - lr: 0.0010\n","Epoch 27/300\n","781/781 [==============================] - 21s 26ms/step - loss: 0.5757 - accuracy: 0.8111 - val_loss: 0.5770 - val_accuracy: 0.8168 - lr: 0.0010\n","Epoch 28/300\n","781/781 [==============================] - 41s 52ms/step - loss: 0.5673 - accuracy: 0.8123 - val_loss: 0.4577 - val_accuracy: 0.8493 - lr: 0.0010\n","Epoch 29/300\n","781/781 [==============================] - 45s 58ms/step - loss: 0.5667 - accuracy: 0.8119 - val_loss: 0.4759 - val_accuracy: 0.8439 - lr: 0.0010\n","Epoch 30/300\n","781/781 [==============================] - 26s 34ms/step - loss: 0.5580 - accuracy: 0.8168 - val_loss: 0.5211 - val_accuracy: 0.8313 - lr: 0.0010\n","Epoch 31/300\n","781/781 [==============================] - 26s 33ms/step - loss: 0.5431 - accuracy: 0.8206 - val_loss: 0.4707 - val_accuracy: 0.8448 - lr: 0.0010\n","Epoch 32/300\n","781/781 [==============================] - 35s 44ms/step - loss: 0.5362 - accuracy: 0.8244 - val_loss: 0.4538 - val_accuracy: 0.8534 - lr: 0.0010\n","Epoch 33/300\n","781/781 [==============================] - 31s 40ms/step - loss: 0.5397 - accuracy: 0.8229 - val_loss: 0.4529 - val_accuracy: 0.8509 - lr: 0.0010\n","Epoch 34/300\n","781/781 [==============================] - 37s 47ms/step - loss: 0.5273 - accuracy: 0.8243 - val_loss: 0.4150 - val_accuracy: 0.8639 - lr: 0.0010\n","Epoch 35/300\n","781/781 [==============================] - 48s 61ms/step - loss: 0.5148 - accuracy: 0.8291 - val_loss: 0.4676 - val_accuracy: 0.8473 - lr: 0.0010\n","Epoch 36/300\n","781/781 [==============================] - 43s 55ms/step - loss: 0.5124 - accuracy: 0.8307 - val_loss: 0.4184 - val_accuracy: 0.8608 - lr: 0.0010\n","Epoch 37/300\n","781/781 [==============================] - 35s 45ms/step - loss: 0.5045 - accuracy: 0.8321 - val_loss: 0.4537 - val_accuracy: 0.8497 - lr: 0.0010\n","Epoch 38/300\n","781/781 [==============================] - 45s 58ms/step - loss: 0.5040 - accuracy: 0.8308 - val_loss: 0.4143 - val_accuracy: 0.8633 - lr: 0.0010\n","Epoch 39/300\n","781/781 [==============================] - 45s 58ms/step - loss: 0.4956 - accuracy: 0.8363 - val_loss: 0.4255 - val_accuracy: 0.8649 - lr: 0.0010\n","Epoch 40/300\n","781/781 [==============================] - 34s 44ms/step - loss: 0.4884 - accuracy: 0.8379 - val_loss: 0.4302 - val_accuracy: 0.8607 - lr: 0.0010\n","Epoch 41/300\n","781/781 [==============================] - 41s 53ms/step - loss: 0.4852 - accuracy: 0.8383 - val_loss: 0.4261 - val_accuracy: 0.8620 - lr: 0.0010\n","Epoch 42/300\n","781/781 [==============================] - 33s 42ms/step - loss: 0.4782 - accuracy: 0.8420 - val_loss: 0.4350 - val_accuracy: 0.8568 - lr: 0.0010\n","Epoch 43/300\n","781/781 [==============================] - 40s 51ms/step - loss: 0.4816 - accuracy: 0.8392 - val_loss: 0.4114 - val_accuracy: 0.8674 - lr: 0.0010\n","Epoch 44/300\n","781/781 [==============================] - 47s 60ms/step - loss: 0.4734 - accuracy: 0.8439 - val_loss: 0.4069 - val_accuracy: 0.8656 - lr: 0.0010\n","Epoch 45/300\n","781/781 [==============================] - 46s 59ms/step - loss: 0.4668 - accuracy: 0.8465 - val_loss: 0.4064 - val_accuracy: 0.8698 - lr: 0.0010\n","Epoch 46/300\n","781/781 [==============================] - 46s 59ms/step - loss: 0.4623 - accuracy: 0.8463 - val_loss: 0.3935 - val_accuracy: 0.8718 - lr: 0.0010\n","Epoch 47/300\n","781/781 [==============================] - 44s 56ms/step - loss: 0.4560 - accuracy: 0.8488 - val_loss: 0.4219 - val_accuracy: 0.8635 - lr: 0.0010\n","Epoch 48/300\n","781/781 [==============================] - 45s 58ms/step - loss: 0.4575 - accuracy: 0.8471 - val_loss: 0.4466 - val_accuracy: 0.8606 - lr: 0.0010\n","Epoch 49/300\n","781/781 [==============================] - 32s 41ms/step - loss: 0.4501 - accuracy: 0.8512 - val_loss: 0.4217 - val_accuracy: 0.8647 - lr: 0.0010\n","Epoch 50/300\n","781/781 [==============================] - 30s 38ms/step - loss: 0.4448 - accuracy: 0.8517 - val_loss: 0.4069 - val_accuracy: 0.8655 - lr: 0.0010\n","Epoch 51/300\n","781/781 [==============================] - 47s 60ms/step - loss: 0.4416 - accuracy: 0.8538 - val_loss: 0.3524 - val_accuracy: 0.8873 - lr: 0.0010\n","Epoch 52/300\n","781/781 [==============================] - 46s 59ms/step - loss: 0.4370 - accuracy: 0.8529 - val_loss: 0.3549 - val_accuracy: 0.8812 - lr: 0.0010\n","Epoch 53/300\n","781/781 [==============================] - 20s 26ms/step - loss: 0.4349 - accuracy: 0.8570 - val_loss: 0.4003 - val_accuracy: 0.8704 - lr: 0.0010\n","Epoch 54/300\n","781/781 [==============================] - 25s 32ms/step - loss: 0.4355 - accuracy: 0.8563 - val_loss: 0.4262 - val_accuracy: 0.8635 - lr: 0.0010\n","Epoch 55/300\n","781/781 [==============================] - 48s 62ms/step - loss: 0.4255 - accuracy: 0.8601 - val_loss: 0.3976 - val_accuracy: 0.8729 - lr: 0.0010\n","Epoch 56/300\n","781/781 [==============================] - 45s 57ms/step - loss: 0.4280 - accuracy: 0.8577 - val_loss: 0.3698 - val_accuracy: 0.8793 - lr: 0.0010\n","Epoch 57/300\n","781/781 [==============================] - 45s 57ms/step - loss: 0.3923 - accuracy: 0.8694 - val_loss: 0.4193 - val_accuracy: 0.8677 - lr: 5.0000e-04\n","Epoch 58/300\n","781/781 [==============================] - 45s 57ms/step - loss: 0.3822 - accuracy: 0.8698 - val_loss: 0.3339 - val_accuracy: 0.8901 - lr: 5.0000e-04\n","Epoch 59/300\n","781/781 [==============================] - 45s 57ms/step - loss: 0.3753 - accuracy: 0.8729 - val_loss: 0.3292 - val_accuracy: 0.8917 - lr: 5.0000e-04\n","Epoch 60/300\n","781/781 [==============================] - 45s 57ms/step - loss: 0.3653 - accuracy: 0.8769 - val_loss: 0.3401 - val_accuracy: 0.8896 - lr: 5.0000e-04\n","Epoch 61/300\n","781/781 [==============================] - 43s 55ms/step - loss: 0.3672 - accuracy: 0.8760 - val_loss: 0.3449 - val_accuracy: 0.8878 - lr: 5.0000e-04\n","Epoch 62/300\n","781/781 [==============================] - 45s 57ms/step - loss: 0.3578 - accuracy: 0.8808 - val_loss: 0.3616 - val_accuracy: 0.8832 - lr: 5.0000e-04\n","Epoch 63/300\n","781/781 [==============================] - 45s 57ms/step - loss: 0.3602 - accuracy: 0.8777 - val_loss: 0.3332 - val_accuracy: 0.8936 - lr: 5.0000e-04\n","Epoch 64/300\n","781/781 [==============================] - 45s 57ms/step - loss: 0.3556 - accuracy: 0.8788 - val_loss: 0.3391 - val_accuracy: 0.8902 - lr: 5.0000e-04\n","Epoch 65/300\n","781/781 [==============================] - 45s 57ms/step - loss: 0.3346 - accuracy: 0.8873 - val_loss: 0.3266 - val_accuracy: 0.8944 - lr: 2.5000e-04\n","Epoch 66/300\n","781/781 [==============================] - 45s 57ms/step - loss: 0.3297 - accuracy: 0.8898 - val_loss: 0.3267 - val_accuracy: 0.8969 - lr: 2.5000e-04\n","Epoch 67/300\n","781/781 [==============================] - 45s 57ms/step - loss: 0.3289 - accuracy: 0.8900 - val_loss: 0.3201 - val_accuracy: 0.8971 - lr: 2.5000e-04\n","Epoch 68/300\n","781/781 [==============================] - 44s 57ms/step - loss: 0.3268 - accuracy: 0.8886 - val_loss: 0.3247 - val_accuracy: 0.8956 - lr: 2.5000e-04\n","Epoch 69/300\n","781/781 [==============================] - 44s 57ms/step - loss: 0.3265 - accuracy: 0.8895 - val_loss: 0.3165 - val_accuracy: 0.8969 - lr: 2.5000e-04\n","Epoch 70/300\n","781/781 [==============================] - 44s 57ms/step - loss: 0.3243 - accuracy: 0.8913 - val_loss: 0.3141 - val_accuracy: 0.8966 - lr: 2.5000e-04\n","Epoch 71/300\n","781/781 [==============================] - 44s 57ms/step - loss: 0.3198 - accuracy: 0.8924 - val_loss: 0.3118 - val_accuracy: 0.9009 - lr: 2.5000e-04\n","Epoch 72/300\n","781/781 [==============================] - 44s 57ms/step - loss: 0.3194 - accuracy: 0.8917 - val_loss: 0.3090 - val_accuracy: 0.8975 - lr: 2.5000e-04\n","Epoch 73/300\n","781/781 [==============================] - 44s 57ms/step - loss: 0.3195 - accuracy: 0.8928 - val_loss: 0.3124 - val_accuracy: 0.8995 - lr: 2.5000e-04\n","Epoch 74/300\n","781/781 [==============================] - 44s 57ms/step - loss: 0.3211 - accuracy: 0.8922 - val_loss: 0.3216 - val_accuracy: 0.8992 - lr: 2.5000e-04\n","Epoch 75/300\n","781/781 [==============================] - 43s 55ms/step - loss: 0.3167 - accuracy: 0.8935 - val_loss: 0.3322 - val_accuracy: 0.8928 - lr: 2.5000e-04\n","Epoch 76/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.3130 - accuracy: 0.8943 - val_loss: 0.3260 - val_accuracy: 0.8937 - lr: 2.5000e-04\n","Epoch 77/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.3088 - accuracy: 0.8959 - val_loss: 0.3270 - val_accuracy: 0.8970 - lr: 2.5000e-04\n","Epoch 78/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.3085 - accuracy: 0.8954 - val_loss: 0.3172 - val_accuracy: 0.8990 - lr: 1.2500e-04\n","Epoch 79/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.3024 - accuracy: 0.8966 - val_loss: 0.3156 - val_accuracy: 0.8995 - lr: 1.2500e-04\n","Epoch 80/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.3012 - accuracy: 0.8976 - val_loss: 0.2977 - val_accuracy: 0.9067 - lr: 1.2500e-04\n","Epoch 81/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2992 - accuracy: 0.8980 - val_loss: 0.3077 - val_accuracy: 0.9024 - lr: 1.2500e-04\n","Epoch 82/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2972 - accuracy: 0.9001 - val_loss: 0.3106 - val_accuracy: 0.9012 - lr: 1.2500e-04\n","Epoch 83/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2956 - accuracy: 0.8994 - val_loss: 0.3104 - val_accuracy: 0.9009 - lr: 1.2500e-04\n","Epoch 84/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2928 - accuracy: 0.9011 - val_loss: 0.3097 - val_accuracy: 0.9019 - lr: 1.2500e-04\n","Epoch 85/300\n","781/781 [==============================] - 36s 47ms/step - loss: 0.2943 - accuracy: 0.8999 - val_loss: 0.3097 - val_accuracy: 0.9003 - lr: 1.2500e-04\n","Epoch 86/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2900 - accuracy: 0.9004 - val_loss: 0.3113 - val_accuracy: 0.8996 - lr: 6.2500e-05\n","Epoch 87/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2894 - accuracy: 0.9016 - val_loss: 0.3004 - val_accuracy: 0.9049 - lr: 6.2500e-05\n","Epoch 88/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2875 - accuracy: 0.9014 - val_loss: 0.2992 - val_accuracy: 0.9058 - lr: 6.2500e-05\n","Epoch 89/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2846 - accuracy: 0.9039 - val_loss: 0.2998 - val_accuracy: 0.9063 - lr: 6.2500e-05\n","Epoch 90/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2853 - accuracy: 0.9032 - val_loss: 0.3070 - val_accuracy: 0.9037 - lr: 6.2500e-05\n","Epoch 91/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2845 - accuracy: 0.9035 - val_loss: 0.3081 - val_accuracy: 0.9032 - lr: 3.1250e-05\n","Epoch 92/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2879 - accuracy: 0.9010 - val_loss: 0.3054 - val_accuracy: 0.9045 - lr: 3.1250e-05\n","Epoch 93/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2827 - accuracy: 0.9045 - val_loss: 0.3024 - val_accuracy: 0.9053 - lr: 3.1250e-05\n","Epoch 94/300\n","781/781 [==============================] - 36s 46ms/step - loss: 0.2861 - accuracy: 0.9026 - val_loss: 0.3003 - val_accuracy: 0.9055 - lr: 3.1250e-05\n","Epoch 95/300\n","781/781 [==============================] - 39s 50ms/step - loss: 0.2858 - accuracy: 0.9028 - val_loss: 0.3036 - val_accuracy: 0.9045 - lr: 3.1250e-05\n","Epoch 96/300\n","781/781 [==============================] - 29s 37ms/step - loss: 0.2780 - accuracy: 0.9059 - val_loss: 0.3041 - val_accuracy: 0.9053 - lr: 1.5625e-05\n","Epoch 97/300\n","781/781 [==============================] - 23s 30ms/step - loss: 0.2837 - accuracy: 0.9037 - val_loss: 0.3026 - val_accuracy: 0.9051 - lr: 1.5625e-05\n","Epoch 98/300\n","781/781 [==============================] - 41s 52ms/step - loss: 0.2850 - accuracy: 0.9019 - val_loss: 0.3005 - val_accuracy: 0.9061 - lr: 1.5625e-05\n","Epoch 99/300\n","781/781 [==============================] - 39s 50ms/step - loss: 0.2831 - accuracy: 0.9026 - val_loss: 0.2983 - val_accuracy: 0.9066 - lr: 1.5625e-05\n","Epoch 100/300\n","781/781 [==============================] - 39s 50ms/step - loss: 0.2833 - accuracy: 0.9048 - val_loss: 0.3003 - val_accuracy: 0.9063 - lr: 1.5625e-05\n","Epoch 101/300\n","781/781 [==============================] - 40s 51ms/step - loss: 0.2803 - accuracy: 0.9037 - val_loss: 0.3011 - val_accuracy: 0.9067 - lr: 7.8125e-06\n","Epoch 102/300\n","781/781 [==============================] - 37s 48ms/step - loss: 0.2784 - accuracy: 0.9063 - val_loss: 0.3010 - val_accuracy: 0.9057 - lr: 7.8125e-06\n","Epoch 103/300\n","781/781 [==============================] - 33s 42ms/step - loss: 0.2823 - accuracy: 0.9033 - val_loss: 0.2993 - val_accuracy: 0.9069 - lr: 7.8125e-06\n","Epoch 104/300\n","781/781 [==============================] - 43s 55ms/step - loss: 0.2748 - accuracy: 0.9067 - val_loss: 0.3016 - val_accuracy: 0.9056 - lr: 7.8125e-06\n","Epoch 105/300\n","781/781 [==============================] - 45s 58ms/step - loss: 0.2825 - accuracy: 0.9042 - val_loss: 0.2997 - val_accuracy: 0.9060 - lr: 7.8125e-06\n","Epoch 106/300\n","781/781 [==============================] - 30s 38ms/step - loss: 0.2764 - accuracy: 0.9060 - val_loss: 0.3001 - val_accuracy: 0.9063 - lr: 3.9063e-06\n","Epoch 107/300\n","781/781 [==============================] - 39s 50ms/step - loss: 0.2818 - accuracy: 0.9031 - val_loss: 0.2978 - val_accuracy: 0.9068 - lr: 3.9063e-06\n","Epoch 108/300\n","781/781 [==============================] - 24s 31ms/step - loss: 0.2769 - accuracy: 0.9048 - val_loss: 0.2995 - val_accuracy: 0.9066 - lr: 3.9063e-06\n","Epoch 109/300\n","781/781 [==============================] - 39s 50ms/step - loss: 0.2794 - accuracy: 0.9057 - val_loss: 0.3026 - val_accuracy: 0.9057 - lr: 3.9063e-06\n","Epoch 110/300\n","781/781 [==============================] - 44s 56ms/step - loss: 0.2853 - accuracy: 0.9026 - val_loss: 0.2976 - val_accuracy: 0.9071 - lr: 3.9063e-06\n","Epoch 111/300\n","781/781 [==============================] - 26s 33ms/step - loss: 0.2824 - accuracy: 0.9037 - val_loss: 0.3008 - val_accuracy: 0.9061 - lr: 1.9531e-06\n","Epoch 112/300\n","781/781 [==============================] - 21s 27ms/step - loss: 0.2829 - accuracy: 0.9054 - val_loss: 0.3025 - val_accuracy: 0.9058 - lr: 1.9531e-06\n","Epoch 113/300\n","781/781 [==============================] - 25s 32ms/step - loss: 0.2819 - accuracy: 0.9036 - val_loss: 0.3008 - val_accuracy: 0.9055 - lr: 1.9531e-06\n","Epoch 114/300\n","781/781 [==============================] - 23s 30ms/step - loss: 0.2827 - accuracy: 0.9034 - val_loss: 0.3009 - val_accuracy: 0.9061 - lr: 1.9531e-06\n","Epoch 115/300\n","781/781 [==============================] - 44s 57ms/step - loss: 0.2795 - accuracy: 0.9053 - val_loss: 0.3006 - val_accuracy: 0.9063 - lr: 1.9531e-06\n","Epoch 116/300\n","781/781 [==============================] - 27s 35ms/step - loss: 0.2810 - accuracy: 0.9051 - val_loss: 0.3014 - val_accuracy: 0.9055 - lr: 1.0000e-06\n","Epoch 117/300\n","781/781 [==============================] - 23s 29ms/step - loss: 0.2777 - accuracy: 0.9062 - val_loss: 0.3006 - val_accuracy: 0.9064 - lr: 1.0000e-06\n","Epoch 118/300\n"," 58/781 [=>............................] - ETA: 43s - loss: 0.2758 - accuracy: 0.9030"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn [5], line 70\u001b[0m\n\u001b[0;32m     66\u001b[0m set_lr \u001b[39m=\u001b[39m RLROP(factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, min_lr\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[39m## TRAINING with DA and LRA\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m history\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mfit(datagen\u001b[39m.\u001b[39;49mflow(x_train, y_train,batch_size\u001b[39m=\u001b[39;49mbatch_size),\n\u001b[0;32m     71\u001b[0m                             steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(x_train) \u001b[39m/\u001b[39;49m batch_size, \n\u001b[0;32m     72\u001b[0m                             epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     73\u001b[0m                             validation_data\u001b[39m=\u001b[39;49m(x_test, y_test),\n\u001b[0;32m     74\u001b[0m                             callbacks\u001b[39m=\u001b[39;49m[set_lr],\n\u001b[0;32m     75\u001b[0m                             verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     78\u001b[0m \u001b[39m## TEST\u001b[39;00m\n\u001b[0;32m     79\u001b[0m scores \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\keras\\engine\\training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1412\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs  \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1414\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1416\u001b[0m   \u001b[39mbreak\u001b[39;00m\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m'\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m'\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. Expected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[39m=\u001b[39m hook_name\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   hook(batch, logs)\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1107\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\keras\\utils\\tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    605\u001b[0m   \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 607\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\keras\\utils\\tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    599\u001b[0m   \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 601\u001b[0m     t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    602\u001b[0m   \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \n\u001b[0;32m   1138\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n","File \u001b[1;32me:\\Anaconda\\envs\\vencevu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1124\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1125\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1126\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["## DEF NN TOPOLOGY  \n","model = Sequential()\n","\n","model.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))\n","model.add(BN())\n","model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(DP(0.3))\n","\n","model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(DP(0.3))\n","\n","model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(DP(0.5))\n","\n","model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(DP(0.5))\n","\n","model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n","model.add(BN())\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(DP(0.5))\n","\n","model.add(Flatten())\n","model.add(Dense(512, activation='relu'))\n","model.add(BN())\n","model.add(DP(0.5))\n","model.add(Dense(num_classes, activation='softmax'))    # num_classes = 10\n","\n","model.summary()\n","\n","\n","## OPTIM AND COMPILE\n","opt = Adam(learning_rate=0.001)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","# DEFINE A LEARNING RATE SCHEDULER\n","set_lr = RLROP(factor=0.5, min_lr=1e-6, patience=5)\n","\n","\n","## TRAINING with DA and LRA\n","history=model.fit(datagen.flow(x_train, y_train,batch_size=batch_size),\n","                            steps_per_epoch=len(x_train) / batch_size, \n","                            epochs=epochs,\n","                            validation_data=(x_test, y_test),\n","                            callbacks=[set_lr],\n","                            verbose=1)\n","\n","\n","## TEST\n","scores = model.evaluate(x_test, y_test, verbose=1)\n","print('Test loss:', scores[0])\n","print('Test accuracy:', scores[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1673515868445,"user":{"displayName":"Vencevú '_'","userId":"04987947938800528309"},"user_tz":-60},"id":"PWQxFKX339DJ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPOEGOdqa/ZgxGO37QwygPa","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"vencevu","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"88a2c52dd2244db0db2d2f6b636ef43ce35273efa1a6ae2eb619080e9fb1309e"}}},"nbformat":4,"nbformat_minor":0}
